{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe2fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvan Baier\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import bz2 \n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cd8d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C105795698.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C111368507.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C12554922.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C144024400.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C153294291.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C184779094.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C8058405.pbz2',\n",
       " 'OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_C97355855.pbz2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read in Files\n",
    "\n",
    "path = os.getcwd()\n",
    "folder = '\\\\Data'\n",
    "files = os.listdir(path + folder)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a779bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>Discipline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2765252368</td>\n",
       "      <td>2017</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W2135405592</td>\n",
       "      <td>2009</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W2116007522</td>\n",
       "      <td>1971</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W2908600692</td>\n",
       "      <td>2019</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W3165125549</td>\n",
       "      <td>2021</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            work_id publication_year Discipline\n",
       "0  https://openalex.org/W2765252368             2017   C8058405\n",
       "1  https://openalex.org/W2135405592             2009   C8058405\n",
       "2  https://openalex.org/W2116007522             1971   C8058405\n",
       "3  https://openalex.org/W2908600692             2019   C8058405\n",
       "4  https://openalex.org/W3165125549             2021   C8058405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['substorm expansion',\n",
       " 'wave frequencies',\n",
       " 'small substorm',\n",
       " 'expansion phase',\n",
       " 'small substorm expansion',\n",
       " 'substorm expansion phase',\n",
       " 'substorm onset',\n",
       " 'characteristics of the onset',\n",
       " 'physics of substorm',\n",
       " 'frequencies concurrent']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender_ROR</th>\n",
       "      <th>Receiver_ROR</th>\n",
       "      <th>Year</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Discipline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ror.org/00hj8s172</td>\n",
       "      <td>https://ror.org/00hj8s172</td>\n",
       "      <td>1966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ror.org/042nb2s44</td>\n",
       "      <td>https://ror.org/00hj8s172</td>\n",
       "      <td>1966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ror.org/016st3p78</td>\n",
       "      <td>https://ror.org/02acart68</td>\n",
       "      <td>1967</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ror.org/027m9bs27</td>\n",
       "      <td>https://ror.org/02acart68</td>\n",
       "      <td>1967</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ror.org/02acart68</td>\n",
       "      <td>https://ror.org/02acart68</td>\n",
       "      <td>1967</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>C8058405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Sender_ROR               Receiver_ROR  Year  Citations  \\\n",
       "0  https://ror.org/00hj8s172  https://ror.org/00hj8s172  1966   1.000000   \n",
       "1  https://ror.org/042nb2s44  https://ror.org/00hj8s172  1966   1.000000   \n",
       "2  https://ror.org/016st3p78  https://ror.org/02acart68  1967   0.090909   \n",
       "3  https://ror.org/027m9bs27  https://ror.org/02acart68  1967   0.250000   \n",
       "4  https://ror.org/02acart68  https://ror.org/02acart68  1967   0.500000   \n",
       "\n",
       "  Discipline  \n",
       "0   C8058405  \n",
       "1   C8058405  \n",
       "2   C8058405  \n",
       "3   C8058405  \n",
       "4   C8058405  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load in cPickle file for Geophysics (OpenAlex ID C8058405)\n",
    "discipline = 'C8058405'\n",
    "\n",
    "\n",
    "\n",
    "# load in cPickle file for Statistics (OpenAlex ID C105795698)\n",
    "#discipline = 'C105795698'\n",
    "\n",
    "# load in cPickle file for Oceanography (OpenAlex ID C111368507)\n",
    "#discipline = 'C111368507'\n",
    "\n",
    "# load in cPickle file for Biophysics (OpenAlex ID C12554922)\n",
    "#discipline = 'C12554922'\n",
    "\n",
    "# load in cPickle file for Meteorology (OpenAlex ID C153294291)\n",
    "#discipline = 'C153294291'\n",
    "\n",
    "# load in cPickle file for Atomic physics (OpenAlex ID C184779094)\n",
    "#discipline = 'C184779094'\n",
    "\n",
    "# load in cPickle file for Thermodynamics (OpenAlex ID C97355855)\n",
    "#discipline = 'C97355855'\n",
    "\n",
    "Data_Packet = 'Data\\OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_' + discipline + '.pbz2'\n",
    "\n",
    "f = bz2.BZ2File(Data_Packet, 'rb')\n",
    "paper_id_year_df = cPickle.load(f)\n",
    "corpus_dict = cPickle.load(f)\n",
    "citation_df = cPickle.load(f)\n",
    "\n",
    "# Object 1: dataframe with all the paper IDs and the year they were published: needed for the corpus_dict\n",
    "geophysics_paper_id_year_df = paper_id_year_df\n",
    "display(geophysics_paper_id_year_df.head())\n",
    "\n",
    "# Object 2: dictionary where the keys are the paper IDs and the values are a list containing the extracted terms\n",
    "# Structure: corpus_dict[Discipline_ID][paper_id] = [term1, term2, term3,...]: incl. eN and non-EN terms\n",
    "geophysics_corpus_dict = corpus_dict\n",
    "\n",
    "# sample call for single work\n",
    "display(geophysics_corpus_dict.get('C8058405').get('https://openalex.org/W2765252368'))\n",
    "\n",
    "# full call for all works\n",
    "#display(next(iter(geophysics_corpus_dict.items())))\n",
    "\n",
    "# Object 3: dataframe that's an edgelist between receiver RORs and sender RORs per year (= research organization registry)\n",
    "display(citation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "753405e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess Data for single field\n",
    "\n",
    "def reformat(dictionary):\n",
    "    '''\n",
    "    Takes the Object 2 dictionary and casts it into a dataframe\n",
    "    '''\n",
    "    # remove OpenAlex ID\n",
    "    field_dictionary = dictionary[1]\n",
    "    # reformat to list\n",
    "    field_list = list(map(list, field_dictionary.items()))\n",
    "    # cast into dataframe and rename columns\n",
    "    field_df = pd.DataFrame(field_list)\n",
    "    field_df = field_df.rename(columns = {0: 'work_id', 1: 'terms'})\n",
    "    \n",
    "    return field_df\n",
    "\n",
    "def add_year(field_df, paper_id_year_df):\n",
    "    '''\n",
    "    Takes the Object 2 dataframe and merges it with corresponding publication years from Object 1\n",
    "    '''\n",
    "    # merge the two dataframes\n",
    "    merged = pd.merge(field_df, paper_id_year_df, on ='work_id', how ='inner')\n",
    "    # reorder columns\n",
    "    columns = merged.columns.tolist()\n",
    "    columns = columns[-1:] + columns[-2:-1] + columns[:-2]\n",
    "    reordered = merged[columns]\n",
    "    \n",
    "    return reordered\n",
    "\n",
    "def lowercase(dataframe, column):\n",
    "    '''\n",
    "    takes a dataframe and lowercases everything within a specified column (column contents must be in a list of strings)\n",
    "    '''\n",
    "    dataframe[column] = dataframe[column].apply(lambda lst: [word.lower() for word in lst])\n",
    "    return dataframe\n",
    "\n",
    "def counts_per_document(reordered):\n",
    "    '''\n",
    "    Add number of documents, terms, unique terms, words, and unique words per document to the dataframe\n",
    "    '''\n",
    "    pd.set_option('mode.chained_assignment',None)\n",
    "    \n",
    "    reordered.loc[:,'NoD_pD'] = 1\n",
    "    reordered.loc[:,'NoT_pD'] = [len(cell) for cell in reordered['terms']]\n",
    "    reordered.loc[:,'NoUT_pD'] = [len(set(cell)) for cell in reordered['terms']]\n",
    "    reordered.loc[:,'NoW_pD'] = [sum([len(term.split()) for term in cell]) for cell in reordered['terms']]\n",
    "    reordered.loc[:,'NoUW_pD'] = [len(set([item for sublist in [term.split() for term in cell]\n",
    "                                           for item in sublist])) for cell in reordered['terms']]\n",
    "    \n",
    "    #display(reordered.describe())\n",
    "    return reordered\n",
    "\n",
    "def counts_per_year(reordered):\n",
    "    '''\n",
    "    Add number of documents, terms, unique terms, words, and unique words per year to the dataframe\n",
    "    '''\n",
    "    # aggregate documents per year and concatenate the list(s) of words\n",
    "    words = reordered.groupby('publication_year', as_index=False)['stemmed_tokens'].agg(lambda x: list(chain.from_iterable(x)))\n",
    "    # aggregate documents per year and count the number of documents\n",
    "    documents = reordered.groupby('publication_year', as_index=False).size()\n",
    "    # put the two dataframes together\n",
    "    grouped = pd.concat([words, documents['size']], axis = 1)\n",
    "    \n",
    "    # get counts of terms and words per year\n",
    "    grouped = grouped.rename(columns = {'size':'NoD'})\n",
    "    #grouped.loc[:,'NoT'] = [len(cell) for cell in grouped['terms']]\n",
    "    #grouped.loc[:,'NoUT'] = [len(set(cell)) for cell in grouped['terms']]\n",
    "    #grouped.loc[:,'NoW'] = [sum([len(term.split()) for term in cell]) for cell in grouped['words']]\n",
    "    #grouped.loc[:,'NoUW'] = [len(set([item for sublist in [term.split() for term in cell]\n",
    "                                      #for item in sublist])) for cell in grouped['words']]\n",
    "    grouped.loc[:,'NoS'] = [sum([len(term.split()) for term in cell]) for cell in grouped['stemmed_tokens']]\n",
    "    grouped.loc[:,'NoUS'] = [len(set([item for sublist in [term.split() for term in cell]\n",
    "                                      for item in sublist])) for cell in grouped['stemmed_tokens']]\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "def split_string(dataframe, column):\n",
    "    '''\n",
    "    Split strings into substrings for a given column in the dataframe, creating the new column 'words'\n",
    "    '''\n",
    "    dataframe['words'] = dataframe[column].apply(lambda lst: [word for line in lst for word in line.split()])\n",
    "    return dataframe\n",
    "\n",
    "def remove_stopwords(dataframe, column):\n",
    "    '''\n",
    "    Remove stopwords from a list of words\n",
    "    '''\n",
    "    dataframe[column] = dataframe[column].apply(lambda lst: [word for word in lst if word not in stopwords])\n",
    "    return dataframe\n",
    "\n",
    "def wordcounter(wordlist, n):\n",
    "    '''\n",
    "    Counts terms/words within a list of strings, returns top n terms/words over time\n",
    "    Idea: Use output as illustrative example of how field progresses (validate with field-specific paper on paradigm shift)\n",
    "    '''\n",
    "    counts = {}\n",
    "    for word in wordlist:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    \n",
    "    # convert dictionary to list of tuples\n",
    "    lst_counts = [(key, value) for key, value in counts.items()]\n",
    "    #sort in descending order\n",
    "    lst_counts.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return lst_counts[:n]\n",
    "\n",
    "def wordcounter_column(dataframe, column, n):\n",
    "    '''\n",
    "    Apply wordcounter() function to the entire column of a dataframe, returns a new column with top n items per year\n",
    "    '''\n",
    "    # define the new column name and fill it with nan values\n",
    "    if n != 1:\n",
    "        new_column = 'top ' + str(n) + ' ' + column\n",
    "    else:\n",
    "        new_column = 'top ' + str(n) + ' ' + column[:-1]\n",
    "    dataframe[new_column] = np.nan\n",
    "    \n",
    "    # loop through each row to get most frequent words\n",
    "    for index, row in dataframe.iterrows():\n",
    "        dataframe.iloc[index,dataframe.columns.get_loc(new_column)] = [wordcounter(row[column], n)]    \n",
    "    \n",
    "    # above line throws an error if outer brackets are removed, the followinf code flattens the nested list\n",
    "    # dataframe[new_column] =  dataframe[new_column].apply(np.ravel)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def wordcounter_abs_and_perc(dataframe, column, n, percentage):\n",
    "    '''\n",
    "    UPDATED VERSION OF WORDCOUNTER_COLUMN\n",
    "    \n",
    "    Apply wordcounter() function to the entire column of a dataframe, returns a new column with either\n",
    "    top n items per year or top n percent of items per year\n",
    "    \n",
    "    Function takes in a dataframe, a column name ('words' or 'terms'), n (either as whole number of cases or as percentage,\n",
    "    and boolean percentage ('YES' or 'NO' to indicate if number is top n or top n percent))\n",
    "    '''    \n",
    "    # define the new column name conditional on percentage\n",
    "    if percentage == 'YES':\n",
    "        # get number of unique words/terms based on given percentage\n",
    "        new_counter = 'NoU' + str(column[0]).capitalize() + ' (t' + str(n) + '%)'        \n",
    "        new_column = 't' + str(n) + '% of ' + column\n",
    "    elif n!= 1:\n",
    "        new_column = 't' + str(n) + ' ' + column\n",
    "    else:\n",
    "        new_column = 't' + str(n) + ' ' + column[:-1]\n",
    "        \n",
    "    # populate new_counter column with an integer of terms, if percentage given\n",
    "    if percentage == 'NO':\n",
    "        pass\n",
    "    #elif column == 'terms':\n",
    "        #dataframe[new_counter] = dataframe['NoUT'].multiply((n/100)).round().astype(np.int64)\n",
    "    #elif column == 'words':\n",
    "        #dataframe[new_counter] = dataframe['NoUW'].multiply((n/100)).round().astype(np.int64)\n",
    "    elif column == 'stemmed_tokens':\n",
    "        dataframe[new_counter] = dataframe['NoUS'].multiply((n/100)).round().astype(np.int64)\n",
    "        \n",
    "    # fill other column with nan values\n",
    "    dataframe[new_column] = np.nan\n",
    "    \n",
    "    # loop through each row to get most frequent words\n",
    "    for index, row in dataframe.iterrows():        \n",
    "        # condition for top n % of terms\n",
    "        if percentage == 'YES':\n",
    "            NoUX = dataframe.iloc[index,dataframe.columns.get_loc(new_counter)]\n",
    "            # account for edge case of NoUT being 0\n",
    "            if NoUX >= 1:\n",
    "                dataframe.iloc[index,dataframe.columns.get_loc(new_column)] = [wordcounter(row[column], NoUX)]\n",
    "            else:\n",
    "                dataframe.iloc[index,dataframe.columns.get_loc(new_column)] = np.nan\n",
    "        # condition for top n terms\n",
    "        else:\n",
    "            dataframe.iloc[index,dataframe.columns.get_loc(new_column)] = [wordcounter(row[column], n)]\n",
    "            \n",
    "        # above line throws an error if outer brackets are removed, the following code flattens the nested list\n",
    "        # dataframe[new_column] =  dataframe[new_column].apply(np.ravel)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a88fd946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "regex = r'[^a-z\\s]'\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Cleans, tokenizes + stems Pandas series of strings    \n",
    "    Returns pandas series of lists of tokens\n",
    "    '''\n",
    "    # Clean text with regex\n",
    "    clean = text.str.lower().str.replace(regex, '', regex=True)\n",
    "\n",
    "    # Anonymous tokenizer + stemmer functions\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    tokenize = lambda text: [i for i in nltk.word_tokenize(text) if i not in stop]\n",
    "    stemmer = lambda tokens: [SnowballStemmer('english').stem(token) for token in tokens]\n",
    "\n",
    "    # Tokenize and stem clean text\n",
    "    tokens = clean.apply(tokenize)\n",
    "    stemmed_tokens = tokens.apply(stemmer)\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3df241c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lowercase(add_year(reformat(next(iter(geophysics_corpus_dict.items()))), geophysics_paper_id_year_df), 'terms')\n",
    "\n",
    "# remove rows with empty lists\n",
    "b = a[a['terms'].map(lambda d: len(d)) > 0]\n",
    "\n",
    "# reformat row as strings\n",
    "b = b.astype({'terms':'string'})\n",
    "\n",
    "# split terms into words and stem\n",
    "b['stemmed_tokens'] = tokenize(b['terms'])\n",
    "\n",
    "# get ocunts per year for documents, words ,adn unique words (now no more terms because of stemming)\n",
    "b = counts_per_year(b)\n",
    "\n",
    "# top 10, 50, 100, 500 unique stemmed tokens\n",
    "c = wordcounter_abs_and_perc(b, 'stemmed_tokens', 10, 'NO')\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 50, 'NO')\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 100, 'NO')\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 500, 'NO')\n",
    "\n",
    "# top 1%, 10%, 20%, 25 % of unique stemmed tokens\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 1, 'YES')\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 10, 'YES')\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 20, 'YES')\n",
    "c = wordcounter_abs_and_perc(c, 'stemmed_tokens', 25, 'YES')\n",
    "\n",
    "# flatten list, calculate len, and divide by 2 since list contains words and their count (should max at 100)\n",
    "#c.loc[:,'t100 tokens count'] = c['t100 words'].apply(np.ravel).apply(len).div(2).astype(np.int64)\n",
    "# select relevant keys and set publication_year to index\n",
    "#d = c[['publication_year', 'NoD', 'NoT', 'NoUT', 'NoW', 'NoUW', 't100 words count', 't100 terms count', 'NoUW (t25%)',\n",
    "      #'NoUT (t25%)']]\n",
    "\n",
    "# fill NaN values with 0 (cases where terms do not meet threshold for meaningful results for a given top percentage)\n",
    "c.fillna(0)\n",
    "\n",
    "# convert publication year back to integer\n",
    "c = c.astype({'publication_year':'int'})\n",
    "\n",
    "d = c.set_index('publication_year')\n",
    "\n",
    "# create complete index without missing years\n",
    "new_index = list(range(int(min(d.index)), int(max(d.index)) + 1))\n",
    "\n",
    "# create empty dataframe with complete index\n",
    "e = pd.DataFrame(np.nan, index = new_index, columns = d.columns)\n",
    "\n",
    "e.index.name = 'publication_year'\n",
    "\n",
    "f = e.combine_first(d)\n",
    "f.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc1d635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_year</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>NoD</th>\n",
       "      <th>NoS</th>\n",
       "      <th>NoUS</th>\n",
       "      <th>t10 stemmed_tokens</th>\n",
       "      <th>t50 stemmed_tokens</th>\n",
       "      <th>t100 stemmed_tokens</th>\n",
       "      <th>t500 stemmed_tokens</th>\n",
       "      <th>NoUS (t1%)</th>\n",
       "      <th>t1% of stemmed_tokens</th>\n",
       "      <th>NoUS (t10%)</th>\n",
       "      <th>t10% of stemmed_tokens</th>\n",
       "      <th>NoUS (t20%)</th>\n",
       "      <th>t20% of stemmed_tokens</th>\n",
       "      <th>NoUS (t25%)</th>\n",
       "      <th>t25% of stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1832</td>\n",
       "      <td>[mean, motion, mean, motion, motion, planet, m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[[(motion, 7), (mean, 6), (planet, 2), (earth,...</td>\n",
       "      <td>[[(motion, 7), (mean, 6), (planet, 2), (earth,...</td>\n",
       "      <td>[[(motion, 7), (mean, 6), (planet, 2), (earth,...</td>\n",
       "      <td>[[(motion, 7), (mean, 6), (planet, 2), (earth,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[(motion, 7)]]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[[(motion, 7), (mean, 6)]]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[[(motion, 7), (mean, 6), (planet, 2)]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2018</td>\n",
       "      <td>[unit, boundari, geostatist, integr, geotechn,...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>3952.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>177.0</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "      <td>222.0</td>\n",
       "      <td>[[(field, 90), (seismic, 67), (model, 57), (ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2019</td>\n",
       "      <td>[lunar, crater, terrestri, crater, crater, dia...</td>\n",
       "      <td>279.0</td>\n",
       "      <td>6103.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>223.0</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "      <td>278.0</td>\n",
       "      <td>[[(model, 94), (field, 85), (structur, 72), (w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2020</td>\n",
       "      <td>[defect, zone, observ, possibl, possibl, defec...</td>\n",
       "      <td>540.0</td>\n",
       "      <td>11527.0</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>157.0</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>314.0</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "      <td>393.0</td>\n",
       "      <td>[[(field, 190), (wave, 168), (model, 141), (ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2021</td>\n",
       "      <td>[ionospher, propag, receiv, amplitud, statist,...</td>\n",
       "      <td>442.0</td>\n",
       "      <td>9556.0</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>141.0</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>281.0</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "      <td>352.0</td>\n",
       "      <td>[[(wave, 156), (field, 154), (model, 124), (ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2022</td>\n",
       "      <td>[anomali, ocean, temperatur, anomali, equatori...</td>\n",
       "      <td>391.0</td>\n",
       "      <td>8420.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>132.0</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>263.0</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "      <td>329.0</td>\n",
       "      <td>[[(model, 171), (field, 146), (wave, 122), (ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     publication_year                                     stemmed_tokens  \\\n",
       "0                1832  [mean, motion, mean, motion, motion, planet, m...   \n",
       "1                1833                                                NaN   \n",
       "2                1834                                                NaN   \n",
       "3                1835                                                NaN   \n",
       "4                1836                                                NaN   \n",
       "..                ...                                                ...   \n",
       "186              2018  [unit, boundari, geostatist, integr, geotechn,...   \n",
       "187              2019  [lunar, crater, terrestri, crater, crater, dia...   \n",
       "188              2020  [defect, zone, observ, possibl, possibl, defec...   \n",
       "189              2021  [ionospher, propag, receiv, amplitud, statist,...   \n",
       "190              2022  [anomali, ocean, temperatur, anomali, equatori...   \n",
       "\n",
       "       NoD      NoS    NoUS  \\\n",
       "0      1.0     25.0    11.0   \n",
       "1      NaN      NaN     NaN   \n",
       "2      NaN      NaN     NaN   \n",
       "3      NaN      NaN     NaN   \n",
       "4      NaN      NaN     NaN   \n",
       "..     ...      ...     ...   \n",
       "186  180.0   3952.0   887.0   \n",
       "187  279.0   6103.0  1113.0   \n",
       "188  540.0  11527.0  1572.0   \n",
       "189  442.0   9556.0  1406.0   \n",
       "190  391.0   8420.0  1316.0   \n",
       "\n",
       "                                    t10 stemmed_tokens  \\\n",
       "0    [[(motion, 7), (mean, 6), (planet, 2), (earth,...   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...   \n",
       "\n",
       "                                    t50 stemmed_tokens  \\\n",
       "0    [[(motion, 7), (mean, 6), (planet, 2), (earth,...   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...   \n",
       "\n",
       "                                   t100 stemmed_tokens  \\\n",
       "0    [[(motion, 7), (mean, 6), (planet, 2), (earth,...   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...   \n",
       "\n",
       "                                   t500 stemmed_tokens  NoUS (t1%)  \\\n",
       "0    [[(motion, 7), (mean, 6), (planet, 2), (earth,...         0.0   \n",
       "1                                                  NaN         NaN   \n",
       "2                                                  NaN         NaN   \n",
       "3                                                  NaN         NaN   \n",
       "4                                                  NaN         NaN   \n",
       "..                                                 ...         ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...         9.0   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...        11.0   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...        16.0   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...        14.0   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...        13.0   \n",
       "\n",
       "                                 t1% of stemmed_tokens  NoUS (t10%)  \\\n",
       "0                                                  NaN          1.0   \n",
       "1                                                  NaN          NaN   \n",
       "2                                                  NaN          NaN   \n",
       "3                                                  NaN          NaN   \n",
       "4                                                  NaN          NaN   \n",
       "..                                                 ...          ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...         89.0   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...        111.0   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...        157.0   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...        141.0   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...        132.0   \n",
       "\n",
       "                                t10% of stemmed_tokens  NoUS (t20%)  \\\n",
       "0                                      [[(motion, 7)]]          2.0   \n",
       "1                                                  NaN          NaN   \n",
       "2                                                  NaN          NaN   \n",
       "3                                                  NaN          NaN   \n",
       "4                                                  NaN          NaN   \n",
       "..                                                 ...          ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...        177.0   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...        223.0   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...        314.0   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...        281.0   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...        263.0   \n",
       "\n",
       "                                t20% of stemmed_tokens  NoUS (t25%)  \\\n",
       "0                           [[(motion, 7), (mean, 6)]]          3.0   \n",
       "1                                                  NaN          NaN   \n",
       "2                                                  NaN          NaN   \n",
       "3                                                  NaN          NaN   \n",
       "4                                                  NaN          NaN   \n",
       "..                                                 ...          ...   \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...        222.0   \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...        278.0   \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...        393.0   \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...        352.0   \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...        329.0   \n",
       "\n",
       "                                t25% of stemmed_tokens  \n",
       "0              [[(motion, 7), (mean, 6), (planet, 2)]]  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "186  [[(field, 90), (seismic, 67), (model, 57), (ma...  \n",
       "187  [[(model, 94), (field, 85), (structur, 72), (w...  \n",
       "188  [[(field, 190), (wave, 168), (model, 141), (ma...  \n",
       "189  [[(wave, 156), (field, 154), (model, 124), (ma...  \n",
       "190  [[(model, 171), (field, 146), (wave, 122), (ma...  \n",
       "\n",
       "[191 rows x 17 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15cd5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_csv('geophysics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EXPLANATION\n",
    "\n",
    "publication_year denotes the year of publication, starting with the earliest available data. For years after the first\n",
    "observation for which data is not available, all entries are coded as NaN\n",
    "\n",
    "stemmed_tokens is the full list of stemmed tokens, including tokens that were used multiple times\n",
    "\n",
    "NoD counts then umber of documents\n",
    "NoS counts the number of stemmed tokens\n",
    "NoUS counts the number of unique stemmed tokens\n",
    "\n",
    "t10 stemmed_tokens, t50 stemmed_tokens, t100 stemmed_tokens, and t500 stemmed_tokens represent the top 10, 50, 100, and 500\n",
    "tokens by frequency, in the format of [('token', frequency count), ('token2', frequency count), etc.]. For years with a\n",
    "number of unique stemmed tokens below the top n, the full number of tokens will be displayed\n",
    "(e.g., 11 tokens for t50 stemmed_tokens in 1832)\n",
    "\n",
    "NoUS (t1%), NoUS (t10%), NoUS (t20%), and NoUS (t25%) count the number of unique stemmed tokens in the top 1, 10, 20, and 25\n",
    "percent. This can be 0 for smaller corpus sizes (e.g., top 1% for 11 unique stemmed tokens is 0, hence NoUS (t1%) is 0)\n",
    "\n",
    "t1% of stemmed_tokens, t10% of stemmed_tokens, t20% of stemmed_tokens, and t25% of stemmed_tokens list the actual tokens\n",
    "with their frequency of occurence, same as t10 stemmed_tokens (and subsequent columns) did before \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
