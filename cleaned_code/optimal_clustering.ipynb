{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports from preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import bz2 \n",
    "import csv\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import numpy as np\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landauer and Dumais Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "YEARS = list(range(2000, 2021))\n",
    "EMBEDDING_DIMS = 10\n",
    "YEAR_COL = \"publication_year\"\n",
    "ID_COL = \"work_id\"\n",
    "OUT_DIR = \"outputs_clustering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes training data, establishing number IDs for each vocabulary word,\n",
    "# converting word sequence into ID sequence (input_as_ids), and providing dict\n",
    "# to map from word to its ID (word2id), and list to map from ID back to word (id2word)\n",
    "def process_training_data(tokens):\n",
    "    \"\"\"Taken from pset 2.\"\"\"\n",
    "    # Create the model's vocabulary and map to unique indices\n",
    "    word2id = {}\n",
    "    id2word = []\n",
    "    for word in tokens:\n",
    "        if word not in word2id:\n",
    "            id2word.append(word)\n",
    "            word2id[word] = len(id2word) - 1\n",
    "    # Convert string of text into string of IDs in a tensor for input to model\n",
    "    input_as_ids = []\n",
    "    for word in tokens:\n",
    "        input_as_ids.append(word2id[word])\n",
    "    # final_ids = torch.LongTensor(input_as_ids)\n",
    "    return input_as_ids,word2id,id2word\n",
    "\n",
    "\n",
    "def generate_mat(tokens, df, corpus_dict):\n",
    "    \"\"\"\n",
    "    tokens: Iterable\n",
    "        set of individual tokens for corpus\n",
    "    df: pd.DataFrame\n",
    "        any subset of the paper_id_year_df\n",
    "    \"\"\"\n",
    "    input_as_ids, word2id, id2word = process_training_data(tokens)\n",
    "    num_unique_tokens = len(id2word)\n",
    "    num_docs = len(df[\"work_id\"])\n",
    "    mat = np.zeros((num_unique_tokens, num_docs))\n",
    "    \n",
    "    for token_idx in range(num_unique_tokens):\n",
    "        for doc_idx, work_id in enumerate(df[\"work_id\"]):\n",
    "            word = id2word[token_idx]\n",
    "            if word in corpus_dict.get(work_id, []):\n",
    "                mat[token_idx, doc_idx] = 1\n",
    "\n",
    "    return mat\n",
    "\n",
    "def svd_dim_reduction(mat):\n",
    "    \"\"\"\n",
    "    TODO: replicate the Landauer and Dumais thing\n",
    "    \"\"\"\n",
    "    u, s, vh = np.linalg.svd(mat)\n",
    "    return None\n",
    "\n",
    "def pca_dim_reduction(mat):\n",
    "    \"\"\"\n",
    "    taken from https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\n",
    "    \"\"\"\n",
    "    # want to reduce on the rows, so take the transpose\n",
    "    A = mat.T\n",
    "    # calculate the mean of each column\n",
    "    M = mean(A.T, axis=1)\n",
    "    # center columns by subtracting column means\n",
    "    C = A - M\n",
    "    # calculate covariance matrix of centered matrix\n",
    "    V = cov(C.T)\n",
    "    # eigendecomposition of covariance matrix\n",
    "    values, vectors = eig(V)\n",
    "    # project data\n",
    "    P = vectors.T.dot(C.T)\n",
    "    return P.T, values\n",
    "\n",
    "def reduce_to_n_dimensions(mat, n):\n",
    "    \"\"\"\n",
    "    mat: np.Array\n",
    "        matrix being reduced\n",
    "    n: int\n",
    "        number of resulting dimensions\n",
    "    \"\"\"\n",
    "    pca_mat, eigenvalues = pca_dim_reduction(mat)\n",
    "    pca_mat = pca_mat.T\n",
    "    abs_eigenvalues = abs(eigenvalues)\n",
    "    sorted_abs = abs_eigenvalues.copy()\n",
    "    sorted_abs.sort()\n",
    "    threshold = sorted_abs[::-1][n]\n",
    "    most_significant = [1 if eigenvalue > threshold else 0 for eigenvalue in abs_eigenvalues]\n",
    "    new_mat = np.array([row for row, sig in zip(pca_mat, most_significant) if sig])\n",
    "    return new_mat\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering with elbow optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.941450686788302"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lsa_reduction(data):\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=EMBEDDING_DIMS), Normalizer(copy=False))\n",
    "    data_lsa = lsa.fit_transform(data)\n",
    "    # explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "    # print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "    return data_lsa\n",
    "\n",
    "def k_means_inertia(data, k):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        max_iter=100,\n",
    "        n_init=10,\n",
    "    )\n",
    "\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    return kmeans.inertia_\n",
    "\n",
    "def slope(p1, p2):\n",
    "    if p1[0] == p2[0]:\n",
    "        print(f\"slope error: points have same x coordinate\")\n",
    "    return (p2[1] - p1[1]) / (p2[0] - p1[0])\n",
    "\n",
    "def perp_slope(p1, p2):\n",
    "    return -1 / slope(p1, p2)\n",
    "\n",
    "def point_dist_to_line(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    line formed from p1 and p2\n",
    "    distance from p3 to the line p1, p2\n",
    "    \"\"\"\n",
    "\n",
    "    m = slope(p1, p2)\n",
    "    m_perp = perp_slope(p1, p2)\n",
    "\n",
    "    # I promise this works, it's just some fancy algebra\n",
    "    x = (1 / (m - m_perp)) * (p3[1] - m_perp * p3[0] - p1[1] + m * p1[0])\n",
    "    y = m * (x - p1[0]) + p1[1]\n",
    "\n",
    "    return np.sqrt((x - p3[0]) ** 2 + (y - p3[1]) ** 2)\n",
    "\n",
    "point_dist_to_line((1, 7), (5, 1), (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C184779094\n",
      "C97355855\n",
      "C12554922\n",
      "C111368507\n",
      "C144024400\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "C105795698\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "C153294291\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "C8058405\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lr/9q868sv54qdfcpvn7vwywvsc0000gn/T/ipykernel_38961/688889742.py:15: ConvergenceWarning: Number of distinct clusters (64) found smaller than n_clusters (65). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = os.getcwd()\n",
    "folder = '/Data'\n",
    "files = os.listdir(path + folder)\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    # load in cPickle file for Geophysics (OpenAlex ID C8058405)\n",
    "    discipline = file.replace(\"OUTPUT_Python_OpenAlex_Citation_and_Abstract_Data_\", \"\").replace(\".pbz2\", \"\")\n",
    "    print(discipline)\n",
    "\n",
    "    outfile = f\"{OUT_DIR}/{discipline}_{EMBEDDING_DIMS}_dims_{YEARS[0]}_{YEARS[-1]}.csv\"\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        continue\n",
    "\n",
    "    Data_Packet = f'Data/{file}'\n",
    "\n",
    "    f = bz2.BZ2File(Data_Packet, 'rb')\n",
    "    paper_id_year_df = cPickle.load(f)\n",
    "    corpus_dict = cPickle.load(f)\n",
    "    citation_df = cPickle.load(f)\n",
    "\n",
    "    corpus_dict = corpus_dict.get(discipline)\n",
    "\n",
    "    optimal_ks = []\n",
    "    num_papers = []\n",
    "    for year in YEARS:\n",
    "        print(year)\n",
    "        ks = []\n",
    "        inertias = []\n",
    "        df_year = paper_id_year_df[paper_id_year_df[YEAR_COL] == str(year)]\n",
    "        \n",
    "        df_year = df_year.sample(n=min(1000, len(df_year[ID_COL])))\n",
    "\n",
    "        num_papers.append(len(df_year[ID_COL]))\n",
    "\n",
    "        tokens = set()\n",
    "        for work_id in df_year[ID_COL]:\n",
    "            new_tokens = set(corpus_dict.get(work_id, set()))\n",
    "            tokens = tokens.union(new_tokens)\n",
    "\n",
    "        mat = generate_mat(tokens, df_year, corpus_dict)\n",
    "        reduced_mat = lsa_reduction(mat.T)\n",
    "        # print(mat.shape)\n",
    "        \n",
    "        for k in range(1, min(101, reduced_mat.shape[0])):\n",
    "            ks.append(k)\n",
    "            inertia = k_means_inertia(reduced_mat, k)\n",
    "            inertias.append(inertia)\n",
    "\n",
    "        first_point = (ks[0], inertias[0])\n",
    "        last_point = (ks[-1], inertias[-1])\n",
    "        distances = []\n",
    "        for k, inertia in zip(ks, inertias):\n",
    "            distances.append(point_dist_to_line(first_point, last_point, (k, inertia)))\n",
    "\n",
    "        optimal_ks.append(ks[np.argmax(distances)])\n",
    "\n",
    "    with open(outfile, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Year\", \"Optimal_Num_Clusters\", \"Num_Papers\"])\n",
    "        for year, op_k, num_p in zip(YEARS, optimal_ks, num_papers):\n",
    "            writer.writerow([year, op_k, num_p])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d82df6d368dc879608945d608abd81a382dda7fccbb6eb2bfbfeebfce0e6aa1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
